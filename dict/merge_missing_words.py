#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os

def merge_missing_words():
    """å°† missing_words.json åˆå¹¶åˆ° dict_en_level_2.json"""
    
    print("ğŸš€ å¼€å§‹åˆå¹¶ missing_words.json åˆ° dict_en_level_2.json...")
    
    # è¯»å– dict_en_level_2.json
    print("ğŸ“– è¯»å– dict_en_level_2.json...")
    with open('dict_en_level_2.json', 'r', encoding='utf-8') as f:
        dict_en_level_2 = json.load(f)
    
    print(f"  è¯»å–åˆ° {len(dict_en_level_2)} ä¸ªå•è¯")
    
    # è¯»å– missing_words.json
    print("ğŸ“– è¯»å– missing_words.json...")
    with open('missing_words.json', 'r', encoding='utf-8') as f:
        missing_words = json.load(f)
    
    print(f"  è¯»å–åˆ° {len(missing_words)} ä¸ªå•è¯")
    
    # åˆ›å»ºç°æœ‰å•è¯çš„é›†åˆï¼ˆç”¨äºå»é‡ï¼‰
    existing_words = set()
    for word_entry in dict_en_level_2:
        existing_words.add(word_entry['character'].lower())
    
    print(f"  ç°æœ‰å•è¯é›†åˆå¤§å°: {len(existing_words)}")
    
    # æ‰¾åˆ°æœ€å¤§ID
    max_id = max(word_entry['id'] for word_entry in dict_en_level_2) if dict_en_level_2 else 0
    print(f"  å½“å‰æœ€å¤§ID: {max_id}")
    
    # åˆå¹¶å•è¯
    new_words = []
    skipped_count = 0
    
    for word_entry in missing_words:
        word = word_entry['character'].lower()
        
        if word in existing_words:
            print(f"  âš ï¸  è·³è¿‡é‡å¤å•è¯: {word_entry['character']}")
            skipped_count += 1
            continue
        
        # åˆ›å»ºæ–°çš„å•è¯æ¡ç›®ï¼Œæ›´æ–°ID
        new_entry = {
            'id': max_id + 1,
            'character': word_entry['character'],
            'phrase': word_entry['phrase'],
            'pinyin': word_entry['pinyin'],
            'sentence': word_entry['sentence']
        }
        
        new_words.append(new_entry)
        existing_words.add(word)
        max_id += 1
        
        print(f"  âœ… æ·»åŠ æ–°å•è¯: {word_entry['character']} (ID: {new_entry['id']})")
    
    # åˆå¹¶æ‰€æœ‰å•è¯
    merged_data = dict_en_level_2 + new_words
    
    # æŒ‰IDæ’åº
    merged_data.sort(key=lambda x: x['id'])
    
    # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶...")
    print(f"  åŸå§‹å•è¯æ•°: {len(dict_en_level_2)}")
    print(f"  æ–°å¢å•è¯æ•°: {len(new_words)}")
    print(f"  è·³è¿‡é‡å¤æ•°: {skipped_count}")
    print(f"  æœ€ç»ˆå•è¯æ•°: {len(merged_data)}")
    
    # åˆ›å»ºå¤‡ä»½
    backup_filename = 'dict_en_level_2_backup.json'
    print(f"ğŸ“‹ åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_filename}")
    with open(backup_filename, 'w', encoding='utf-8') as f:
        json.dump(dict_en_level_2, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
    with open('dict_en_level_2.json', 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… åˆå¹¶å®Œæˆ!")
    print(f"ğŸ“ åŸæ–‡ä»¶å·²å¤‡ä»½ä¸º: {backup_filename}")
    print(f"ğŸ“ åˆå¹¶åçš„æ–‡ä»¶: dict_en_level_2.json")
    
    # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  åŸå§‹å•è¯æ•°: {len(dict_en_level_2)}")
    print(f"  æ–°å¢å•è¯æ•°: {len(new_words)}")
    print(f"  è·³è¿‡é‡å¤æ•°: {skipped_count}")
    print(f"  æœ€ç»ˆå•è¯æ•°: {len(merged_data)}")
    
    # æ˜¾ç¤ºæ–°å¢å•è¯çš„ç¤ºä¾‹
    if new_words:
        print(f"\nğŸ“ æ–°å¢å•è¯ç¤ºä¾‹:")
        for i, word_entry in enumerate(new_words[:5]):
            print(f"  {word_entry['id']:3d}. {word_entry['character']}: {word_entry['phrase']}")
        if len(new_words) > 5:
            print(f"  ... è¿˜æœ‰ {len(new_words) - 5} ä¸ªå•è¯")

if __name__ == "__main__":
    merge_missing_words() 