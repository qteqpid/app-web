#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json

def clear_sentences():
    """æ¸…ç©ºmissing_words.jsonä¸­çš„sentenceå­—æ®µ"""
    
    print("æ­£åœ¨è¯»å– missing_words.json...")
    with open('missing_words.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"è¯»å–åˆ° {len(data)} ä¸ªå•è¯")
    
    # æ¸…ç©ºæ¯ä¸ªå•è¯çš„sentenceå­—æ®µ
    for i, word_entry in enumerate(data):
        data[i]['sentence'] = ""
        print(f"å·²æ¸…ç©ºç¬¬ {i+1:3d}/{len(data)} ä¸ªå•è¯çš„ä¾‹å¥: {word_entry['character']}")
    
    # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
    print("\næ­£åœ¨ä¿å­˜æ–‡ä»¶...")
    with open('missing_words.json', 'w', encoding='utf-8') as f:
        f.write('[\n')
        for i, entry in enumerate(data):
            line = json.dumps(entry, ensure_ascii=False, separators=(',', ':'))
            if i < len(data) - 1:
                line += ','
            f.write(line + '\n')
        f.write(']\n')
    
    print(f"\nâœ… å·²æˆåŠŸæ¸…ç©º {len(data)} ä¸ªå•è¯çš„ä¾‹å¥")
    print("ğŸ“ æ–‡ä»¶å·²ä¿å­˜ä¸º missing_words.json")

if __name__ == "__main__":
    clear_sentences() 